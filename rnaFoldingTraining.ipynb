{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "053f5b86",
   "metadata": {},
   "source": [
    "packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e9c8460",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install torch\n",
    "# !pip install matplotlib\n",
    "# !pip install \"numpy<2\n",
    "# !pip install torch_geometric\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6077209b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import re\n",
    "from time import time\n",
    "import numpy as np\n",
    "import os\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    !rm rnaModel.py\n",
    "    !wget https://raw.githubusercontent.com/SomeoneNamedCaz/RNAFolding/refs/heads/main/rnaModel.py\n",
    "import importlib\n",
    "import rnaModel\n",
    "importlib.reload(rnaModel)\n",
    "from glob import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd966cc",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<accelerate.accelerator.Accelerator object at 0x138bcf790>\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "hiddenDim = 64\n",
    "batch_size = 16\n",
    "outputDim = 1 # one dist for each output TODO: try onehot outputs\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# deepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_accumulation_steps=2)\n",
    "# accelerator = Accelerator(mixed_precision='fp16', deepspeed_plugin=deepspeed_plugin)\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# device = accelerator.device#\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(accelerator)\n",
    "DISTOGRAM_FILE_NAME = \"distogram.pt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2132ce",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b6c2cb4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# input processing\n",
    "\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    DISTOGRAM_DIR = \"/kaggle/input/distograms/distograms\"\n",
    "    inputDir = \"/kaggle/input/\"\n",
    "elif platform == \"darwin\":\n",
    "    inputDir = \".\"\n",
    "    DISTOGRAM_DIR = \"distograms\"\n",
    "trainSeqs = pd.read_csv(os.path.join(inputDir,\"stanford-rna-3d-folding/train_sequences.csv\"))\n",
    "\n",
    "\n",
    "\n",
    "nucs = functools.reduce(set.union,trainSeqs[\"sequence\"].apply(list).map(set).to_list()) # get all unique nucs at all positions\n",
    "nucs.add(\"NONE\")\n",
    "nucToIdx = {nuc: i for i, nuc in enumerate(nucs)}\n",
    "\n",
    "def processSeqDF(inputSeqs,nucToIdx):\n",
    "    indexSeqs = inputSeqs[\"sequence\"].apply(list)\n",
    "    indexSeqs = indexSeqs.apply(pd.Series).fillna(\"NONE\")\n",
    "    idxToKeep = torch.tensor((indexSeqs != 'NONE').to_numpy(int),dtype=int).unsqueeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "    indexSeqs = indexSeqs.map(lambda nuc: nucToIdx[nuc])\n",
    "    indexSeqs.index = inputSeqs[\"target_id\"]\n",
    "\n",
    "    seqTensor = torch.tensor(indexSeqs.values, dtype=torch.int)\n",
    "    return seqTensor, indexSeqs, idxToKeep\n",
    "\n",
    "trainSeqTensor,_ , trainIdxToKeep = processSeqDF(trainSeqs, nucToIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b559522",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# output label processing\n",
    "\n",
    "trainLabels = pd.read_csv(os.path.join(inputDir,\"stanford-rna-3d-folding/train_labels.csv\"))\n",
    "trainLabels[\"ID\"] = trainLabels[\"ID\"].map(lambda x: re.sub(r\"_\\d+$\",\"\",x))\n",
    "pivotedLabels = trainLabels.pivot(index=\"ID\",columns=\"resid\",values=[\"x_1\",\"y_1\",\"z_1\"])\n",
    "pivotedLabels.fillna(0,inplace=True)\n",
    "pivotedLabels.shape\n",
    "labelTensor = torch.tensor(pivotedLabels.to_numpy().reshape(pivotedLabels.shape[0],3,-1)).transpose(1,2)\n",
    "\n",
    "\n",
    "# labelTensor = labelTensor - torch.concat((torch.zeros_like(labelTensor[:,:1]), labelTensor[:,:-1]), dim=1)\n",
    "\n",
    "def converToDistogramSlow(allData):\n",
    "    \"\"\" coords tensor [residue number, coord index(0->x,1->y,2->z)]\n",
    "            returns: tensor [residue number,residue number] # distances between two points\n",
    "    \"\"\" \n",
    "    \n",
    "    for entryIndex, coords in enumerate(allData):\n",
    "        for resIdx, residue in enumerate(coords):\n",
    "            # print(help(torch.linalg.vector_norm))\n",
    "            distancesToResidue = torch.linalg.vector_norm(coords - residue, dim=-1).unsqueeze(0)\n",
    "            if resIdx == 0:\n",
    "                entryDistances = distancesToResidue\n",
    "            else:\n",
    "                entryDistances = torch.concat((entryDistances,distancesToResidue))\n",
    "        entryDistances = entryDistances.unsqueeze(0)\n",
    "        if entryIndex == 0:\n",
    "            allDistances = entryDistances\n",
    "        else:\n",
    "            allDistances = torch.concat((allDistances,entryDistances))\n",
    "        \n",
    "    return allDistances\n",
    "\n",
    "def convertToDistogramFast(coords):\n",
    "    \"\"\" coords tensor [residue number, coord index(0->x,1->y,2->z)]\n",
    "            returns: tensor [residue number,residue number] # distances between two points\n",
    "    \"\"\"   \n",
    "\n",
    "    diff = coords[:,:,None] - coords[:,None,:]\n",
    "    dists = torch.linalg.vector_norm(diff,dim=-1)\n",
    "    dists = dists.half()\n",
    "    return dists\n",
    "\n",
    "assert (convertToDistogramFast(labelTensor[:10,:20]) == converToDistogramSlow(labelTensor[:10,:20]).half()).all()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5ca1752",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def makeDistogramDir(path, seqTensor, idxToKeep, remakeAll=False):\n",
    "    os.makedirs(path, exist_ok=True,)\n",
    "    t0 = time()\n",
    "    for i, _ in enumerate(labelTensor):\n",
    "        \n",
    "        savePath = os.path.join(path,re.sub(r\"\\.(\\w+)$\", str(i) + r\".\\1\",DISTOGRAM_FILE_NAME))\n",
    "        if not os.path.exists(savePath) or remakeAll:\n",
    "            out = convertToDistogramFast(labelTensor[i].unsqueeze(-1))\n",
    "            torch.save((seqTensor[i], idxToKeep[i], out), savePath)\n",
    "    print(\"took\",time()-t0,\"to make distograms\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2465e842",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# makeDistogramDir(DISTOGRAM_DIR,trainSeqTensor, trainIdxToKeep, remakeAll=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9840250",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, seqTensor, idxToKeep, distogramDir):\n",
    "        super().__init__()\n",
    "        self.distogramPaths = glob(os.path.join(distogramDir,\"*\"))\n",
    "        self.seqTensor = seqTensor\n",
    "        self.idxToKeep = idxToKeep\n",
    "    def __len__(self):\n",
    "        return len(self.distogramPaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.load(self.distogramPaths[idx]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39df809a",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.510048 MB\n",
      "29.020096 MB\n"
     ]
    }
   ],
   "source": [
    "print(trainSeqTensor.numel() * trainSeqTensor.element_size() / 1e6, \"MB\")\n",
    "print(trainIdxToKeep.numel() * trainIdxToKeep.element_size() / 1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0042e6ca",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# make data loader\n",
    "train_dataset = RNADataset(trainSeqTensor, trainIdxToKeep, DISTOGRAM_DIR)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a392eaac",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## test input processing \n",
    "testSeqs = pd.read_csv(os.path.join(inputDir,\"stanford-rna-3d-folding/test_sequences.csv\"))\n",
    "testSeqTensor, indexSeqs, testIndexToKeep = processSeqDF(testSeqs, nucToIdx)\n",
    "test_loader = DataLoader(list(zip(testSeqTensor,trainIdxToKeep, testIndexToKeep)), batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29547556",
   "metadata": {},
   "source": [
    "# model architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49b74ab2",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plotRollingAvg(losses,window = 100):\n",
    "    plt.plot(losses,alpha=0.5)\n",
    "    \n",
    "    plt.plot([sum(losses[i:i+window]) / len(losses[i:i+window])  for i in range(len(losses))],color=\"b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d0e9318",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, opter, train_loader,numEpochs = 100,logFreq = 5):\n",
    "    losses = []\n",
    "    try:\n",
    "        with open(\"trainingLog.txt\", \"a\") as logFile:\n",
    "            for epoch in range(numEpochs):\n",
    "                for step, (batchSeqs, batchIdx, batchLabels) in enumerate(train_loader):\n",
    "                    # batchSeqs = batchSeqs.to(device)\n",
    "                    # batchLabels = batchLabels.to(device)\n",
    "                    # batchIdx = batchIdx.to(device)\n",
    "                    opter.zero_grad()\n",
    "                    \n",
    "                    \n",
    "                    seqLens = torch.sum(batchIdx,axis=1).squeeze()\n",
    "\n",
    "                    maxLen = min(max(seqLens),300)\n",
    "                    batchSeqs = batchSeqs[:,:maxLen]\n",
    "                    batchIdx = batchIdx[:,:maxLen].unsqueeze(2)\n",
    "                    print(\"index\",batchIdx.shape)\n",
    "                    batchLabels = batchLabels.transpose(1,-1) # TODO: remake dataset with this included and remove this line\n",
    "                    batchLabels = batchLabels[:,:maxLen,:maxLen]\n",
    "                    print(\"batch labels\", batchLabels.shape)\n",
    "                    # MSE that accounts for differing lengths. This makes short sequences have a similar error to long sequences and zeros out errors on non existant positions\n",
    "                    mse = (batchLabels - model(batchSeqs)) ** 2\n",
    "\n",
    "                    print(\"Outside: input size\", batchSeqs.size(),\"output_size\", mse.size())\n",
    "                    loss = torch.mean(torch.sum(mse * batchIdx,dim=(1,2,3)) / seqLens)\n",
    "                    \n",
    "                    losses.append(loss.item())\n",
    "                    # loss.backward()\n",
    "                    accelerator.backward(loss)\n",
    "                    opter.step()\n",
    "\n",
    "                    if step % logFreq == 0:\n",
    "                        print(\"epoch\", epoch,\"step\", step, torch.mean(torch.tensor(losses[-logFreq:])), file=logFile,flush=True)\n",
    "                        print(\"epoch\", epoch,\"step\", step, torch.mean(torch.tensor(losses[-logFreq:])),flush=True)\n",
    "                        torch.save(model, \"model.pt\")\n",
    "    except KeyboardInterrupt:\n",
    "        return losses\n",
    "    # except:\n",
    "    #     torch.save(model, \"model.pt\")\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "659e2440",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made model with 692609 parameters of which 692609 are trainable\n",
      "index torch.Size([16, 300, 1, 1])\n",
      "batch labels torch.Size([16, 300, 300, 1])\n",
      "inner size torch.Size([16, 300])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m opter \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     11\u001b[0m model, opter, data \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mprepare(model, opter, train_loader)\n\u001b[0;32m---> 12\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, opter, train_loader, numEpochs, logFreq)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch labels\u001b[39m\u001b[38;5;124m\"\u001b[39m, batchLabels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# MSE that accounts for differing lengths. This makes short sequences have a similar error to long sequences and zeros out errors on non existant positions\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m mse \u001b[38;5;241m=\u001b[39m (batchLabels \u001b[38;5;241m-\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatchSeqs\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutside: input size\u001b[39m\u001b[38;5;124m\"\u001b[39m, batchSeqs\u001b[38;5;241m.\u001b[39msize(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, mse\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msum(mse \u001b[38;5;241m*\u001b[39m batchIdx,dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m/\u001b[39m seqLens)\n",
      "File \u001b[0;32m~/dev/rnaFolding/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/rnaFolding/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/rnaFolding/rnaModel.py:35\u001b[0m, in \u001b[0;36mRNAModel.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# input = input.to(self.device)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtBlock1(output) \u001b[38;5;66;03m# has shape batch, seqLen, Channels\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print(\"after transformer\",process.memory_info().rss / 1e6,\"MB\")\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/rnaFolding/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/rnaFolding/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/rnaFolding/.venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/rnaFolding/.venv/lib/python3.9/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "numDistinctInputs = len(nucs)\n",
    "model = rnaModel.RNAModel(numDistinctInputs, hiddenDim, hiddenDim, outputDim)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   model = nn.DataParallel(model)\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "opter = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "model, opter, data = accelerator.prepare(model, opter, train_loader)\n",
    "losses = train(model, opter, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50ae31",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plotRollingAvg(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08015c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/cazcullimore/Downloads/trainingLog.txt\") as file:\n",
    "    data = \"\".join(file.readlines())\n",
    "    floatLosses = [float(elt) for elt in re.findall(r\"\\((\\d+\\.\\d*)\\)\",data)]\n",
    "    plotRollingAvg(floatLosses)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12276181,
     "sourceId": 87793,
     "sourceType": "competition"
    },
    {
     "datasetId": 7320630,
     "sourceId": 11664655,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
