{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":87793,"databundleVersionId":12276181,"sourceType":"competition"},{"sourceId":11664655,"sourceType":"datasetVersion","datasetId":7320630}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"053f5b86","cell_type":"markdown","source":"packages used","metadata":{}},{"id":"5e9c8460","cell_type":"code","source":"# !pip install pandas\n# !pip install torch\n# !pip install matplotlib\n# !pip install \"numpy<2\n# !pip install torch_geometric\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6077209b","cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport functools\nimport re\nfrom time import time\nimport numpy as np\nimport os\nfrom sys import platform\nif platform == \"linux\" or platform == \"linux2\":\n    !rm rnaModel.py\n    !wget https://raw.githubusercontent.com/SomeoneNamedCaz/RNAFolding/refs/heads/main/rnaModel.py\nimport importlib\nimport rnaModel\nimportlib.reload(rnaModel)\nfrom glob import glob\nimport sys","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ddd966cc","cell_type":"code","source":"# hyperparameters\nhiddenDim = 128\nbatch_size = 16\noutputDim = 1 # one dist for each output TODO: try onehot outputs\nlearning_rate = 0.0001\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\nDISTOGRAM_FILE_NAME = \"distogram.pt\"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0d2132ce","cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"id":"6b6c2cb4","cell_type":"code","source":"\n# input processing\n\nif platform == \"linux\" or platform == \"linux2\":\n    DISTOGRAM_DIR = \"/kaggle/input/distograms/distograms\"\n    inputDir = \"/kaggle/input/\"\nelif platform == \"darwin\":\n    inputDir = \".\"\n    DISTOGRAM_DIR = \"distograms\"\ntrainSeqs = pd.read_csv(os.path.join(inputDir,\"stanford-rna-3d-folding/train_sequences.csv\"))\n\n\n\nnucs = functools.reduce(set.union,trainSeqs[\"sequence\"].apply(list).map(set).to_list()) # get all unique nucs at all positions\nnucs.add(\"NONE\")\nnucToIdx = {nuc: i for i, nuc in enumerate(nucs)}\n\ndef processSeqDF(inputSeqs,nucToIdx):\n    indexSeqs = inputSeqs[\"sequence\"].apply(list)\n    indexSeqs = indexSeqs.apply(pd.Series).fillna(\"NONE\")\n    idxToKeep = torch.tensor((indexSeqs != 'NONE').to_numpy(int),dtype=int).unsqueeze(-1)\n\n\n\n    indexSeqs = indexSeqs.map(lambda nuc: nucToIdx[nuc])\n    indexSeqs.index = inputSeqs[\"target_id\"]\n\n    seqTensor = torch.tensor(indexSeqs.values, dtype=torch.int)\n    return seqTensor, indexSeqs, idxToKeep\n\ntrainSeqTensor,_ , trainIdxToKeep = processSeqDF(trainSeqs, nucToIdx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3b559522","cell_type":"code","source":"# output label processing\n\ntrainLabels = pd.read_csv(os.path.join(inputDir,\"stanford-rna-3d-folding/train_labels.csv\"))\ntrainLabels[\"ID\"] = trainLabels[\"ID\"].map(lambda x: re.sub(r\"_\\d+$\",\"\",x))\npivotedLabels = trainLabels.pivot(index=\"ID\",columns=\"resid\",values=[\"x_1\",\"y_1\",\"z_1\"])\npivotedLabels.fillna(0,inplace=True)\npivotedLabels.shape\nlabelTensor = torch.tensor(pivotedLabels.to_numpy().reshape(pivotedLabels.shape[0],3,-1)).transpose(1,2)\n\n\n# labelTensor = labelTensor - torch.concat((torch.zeros_like(labelTensor[:,:1]), labelTensor[:,:-1]), dim=1)\n\ndef converToDistogramSlow(allData):\n    \"\"\" coords tensor [residue number, coord index(0->x,1->y,2->z)]\n            returns: tensor [residue number,residue number] # distances between two points\n    \"\"\" \n    \n    for entryIndex, coords in enumerate(allData):\n        for resIdx, residue in enumerate(coords):\n            # print(help(torch.linalg.vector_norm))\n            distancesToResidue = torch.linalg.vector_norm(coords - residue, dim=-1).unsqueeze(0)\n            if resIdx == 0:\n                entryDistances = distancesToResidue\n            else:\n                entryDistances = torch.concat((entryDistances,distancesToResidue))\n        entryDistances = entryDistances.unsqueeze(0)\n        if entryIndex == 0:\n            allDistances = entryDistances\n        else:\n            allDistances = torch.concat((allDistances,entryDistances))\n        \n    return allDistances\n\ndef convertToDistogramFast(coords):\n    \"\"\" coords tensor [residue number, coord index(0->x,1->y,2->z)]\n            returns: tensor [residue number,residue number] # distances between two points\n    \"\"\"   \n\n    diff = coords[:,:,None] - coords[:,None,:]\n    dists = torch.linalg.vector_norm(diff,dim=-1)\n    dists = dists.half()\n    return dists\n\nassert (convertToDistogramFast(labelTensor[:10,:20]) == converToDistogramSlow(labelTensor[:10,:20]).half()).all()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f5ca1752","cell_type":"code","source":"def makeDistogramDir(path, seqTensor, idxToKeep, remakeAll=False):\n    os.makedirs(path, exist_ok=True,)\n    t0 = time()\n    for i, _ in enumerate(labelTensor):\n        \n        savePath = os.path.join(path,re.sub(r\"\\.(\\w+)$\", str(i) + r\".\\1\",DISTOGRAM_FILE_NAME))\n        if not os.path.exists(savePath) or remakeAll:\n            out = convertToDistogramFast(labelTensor[i].unsqueeze(-1))\n            torch.save((seqTensor[i], idxToKeep[i], out), savePath)\n    print(\"took\",time()-t0,\"to make distograms\")\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2465e842","cell_type":"code","source":"# makeDistogramDir(DISTOGRAM_DIR,trainSeqTensor, trainIdxToKeep, remakeAll=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f9840250","cell_type":"code","source":"class RNADataset(Dataset):\n    def __init__(self, seqTensor, idxToKeep, distogramDir):\n        super().__init__()\n        self.distogramPaths = glob(os.path.join(distogramDir,\"*\"))\n        self.seqTensor = seqTensor\n        self.idxToKeep = idxToKeep\n    def __len__(self):\n        return len(self.distogramPaths)\n\n    def __getitem__(self, idx):\n        return torch.load(self.distogramPaths[idx]) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"39df809a","cell_type":"code","source":"print(trainSeqTensor.numel() * trainSeqTensor.element_size() / 1e6, \"MB\")\nprint(trainIdxToKeep.numel() * trainIdxToKeep.element_size() / 1e6, \"MB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0042e6ca","cell_type":"code","source":"# make data loader\ntrain_dataset = RNADataset(trainSeqTensor, trainIdxToKeep, DISTOGRAM_DIR)\n\ntrain_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a392eaac","cell_type":"code","source":"## test input processing \ntestSeqs = pd.read_csv(os.path.join(inputDir,\"stanford-rna-3d-folding/test_sequences.csv\"))\ntestSeqTensor, indexSeqs, testIndexToKeep = processSeqDF(testSeqs, nucToIdx)\ntest_loader = DataLoader(list(zip(testSeqTensor,trainIdxToKeep, testIndexToKeep)), batch_size=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"29547556","cell_type":"markdown","source":"# model architecture and training","metadata":{}},{"id":"49b74ab2","cell_type":"code","source":"def plotRollingAvg(losses,window = 100):\n    plt.plot(losses,alpha=0.5)\n    \n    plt.plot([sum(losses[i:i+window]) / len(losses[i:i+window])  for i in range(len(losses))],color=\"b\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7d0e9318","cell_type":"code","source":"def train(model, opter, train_loader,numEpochs = 100,logFreq = 5):\n    losses = []\n    try:\n        with open(\"trainingLog.txt\", \"a\") as logFile:\n            for epoch in range(numEpochs):\n                for step, (batchSeqs, batchIdx, batchLabels) in enumerate(train_loader):\n                    batchSeqs = batchSeqs.to(device)\n                    batchLabels = batchLabels.to(device)\n                    batchIdx = batchIdx.to(device)\n                    opter.zero_grad()\n                    \n                    \n                    seqLens = torch.sum(batchIdx,axis=1).squeeze()\n\n                    maxLen = min(max(seqLens),300)\n                    batchSeqs = batchSeqs[:,:maxLen]\n                    batchIdx = batchIdx[:,:maxLen].unsqueeze(2)\n                    print(\"index\",batchIdx.shape)\n                    batchLabels = batchLabels.transpose(1,-1) # TODO: remake dataset with this included and remove this line\n                    batchLabels = batchLabels[:,:maxLen,:maxLen]\n                    print(\"batch labels\", batchLabels.shape)\n                    # MSE that accounts for differing lengths. This makes short sequences have a similar error to long sequences and zeros out errors on non existant positions\n                    mse = (batchLabels - model(batchSeqs)) ** 2\n\n                    print(\"Outside: input size\", batchSeqs.size(),\"output_size\", mse.size())\n                    loss = torch.mean(torch.sum(mse * batchIdx,dim=(1,2,3)) / seqLens)\n                    \n                    losses.append(loss.item())\n                    loss.backward()\n                    opter.step()\n\n                    if step % logFreq == 0:\n                        print(\"epoch\", epoch,\"step\", step, torch.mean(torch.tensor(losses[-logFreq:])), file=logFile,flush=True)\n                        print(\"epoch\", epoch,\"step\", step, torch.mean(torch.tensor(losses[-logFreq:])),flush=True)\n                        torch.save(model, \"model.pt\")\n    except KeyboardInterrupt:\n        return losses\n    # except:\n    #     torch.save(model, \"model.pt\")\n    return losses\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"659e2440","cell_type":"code","source":"numDistinctInputs = len(nucs)\nmodel = rnaModel.RNAModel(numDistinctInputs, hiddenDim, outputDim, device)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  model = nn.DataParallel(model)\n\n# model.to(device)\nopter = torch.optim.Adam(model.parameters(),lr=learning_rate)\nlosses = train(model, opter, train_loader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9f50ae31","cell_type":"code","source":"# plotRollingAvg(losses)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0b08015c","cell_type":"code","source":"with open(\"/Users/cazcullimore/Downloads/trainingLog.txt\") as file:\n    data = \"\".join(file.readlines())\n    floatLosses = [float(elt) for elt in re.findall(r\"\\((\\d+\\.\\d*)\\)\",data)]\n    plotRollingAvg(floatLosses)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}